# ğŸ¯ MonolithicDetectionScreen.js - GuÃ­a Completa

**VersiÃ³n:** 1.0
**Estado:** âœ… Completo y funcional
**PropÃ³sito:** Debugging y comprensiÃ³n del pipeline end-to-end
**Fecha:** 2025-11-13

---

## ğŸ“– Ãndice

1. [Â¿Por quÃ© MonolÃ­tica?](#por-quÃ©-monolÃ­tica)
2. [Estructura General](#estructura-general)
3. [Flujo End-to-End](#flujo-end-to-end)
4. [Componentes Clave](#componentes-clave)
5. [CÃ³mo Usar](#cÃ³mo-usar)
6. [Debugging](#debugging)
7. [MigraciÃ³n a ProducciÃ³n](#migraciÃ³n-a-producciÃ³n)

---

## ğŸ¤” Â¿Por quÃ© MonolÃ­tica?

### Ventajas âœ…

- **TODO EN UN ARCHIVO:** No necesitas saltar entre 5 archivos para entender el flujo
- **SIN ABSTRACCIONES:** Ves exactamente quÃ© estÃ¡ pasando en cada paso
- **DEBUGGING VISUAL:** Logs integrados muestran cada etapa del pipeline
- **EDUCATIVO:** Perfecto para entender cÃ³mo funciona el sistema
- **TESTING RÃPIDO:** Modifica un parÃ¡metro y ves el resultado inmediatamente

### Desventajas âŒ

- **DIFÃCIL DE MANTENER:** El cÃ³digo estÃ¡ repetido
- **ACOPLAMIENTO FUERTE:** Cambios en una parte afectan todo
- **NO ESCALABLE:** Imposible de reutilizar en otros componentes
- **PERFORMANCE:** Sin optimizaciones de hooks

---

## ğŸ—ï¸ Estructura General

```javascript
const MonolithicDetectionScreen = () => {
  // 1ï¸âƒ£ STATE (lÃ­neas 77-110)
  //    - Modelo, MediaPipe, CÃ¡mara, Buffer, UI, Referencias

  // 2ï¸âƒ£ UTILIDADES (lÃ­neas 115-175)
  //    - log(), normalizeKeypoints(), combineHandKeypoints()
  //    - addFrameToBuffer()

  // 3ï¸âƒ£ INICIALIZACIÃ“N (lÃ­neas 179-280)
  //    - loadLabelsFromAsset()
  //    - loadTensorFlowModel()
  //    - initializeMediaPipe()
  //    - useEffect para carga inicial

  // 4ï¸âƒ£ DETECCIÃ“N (lÃ­neas 284-450)
  //    - detectHandsInFrame()
  //    - predictWithModel()
  //    - startDetectionLoop()
  //    - handleStartDetection()
  //    - handleStopDetection()

  // 5ï¸âƒ£ RENDER (lÃ­neas 454-600)
  //    - UI Principal

  return (...)
}
```

---

## ğŸ”„ Flujo End-to-End

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. INICIALIZACIÃ“N (Al montar componente)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  loadTensorFlowModel()                                       â”‚
â”‚   â”œâ”€ Cargar etiquetas desde assets/model/labels.json       â”‚
â”‚   â”œâ”€ Cargar modelo desde assets/model/tfjs_model/model.jsonâ”‚
â”‚   â”œâ”€ Validar arquitectura [1, 24, 126] â†’ [1, 67]          â”‚
â”‚   â””â”€ Warmup: una predicciÃ³n dummy                          â”‚
â”‚                                                              â”‚
â”‚  initializeMediaPipe()                                      â”‚
â”‚   â”œâ”€ Importar @mediapipe/tasks-vision                      â”‚
â”‚   â”œâ”€ Crear HandLandmarker con configuraciÃ³n                â”‚
â”‚   â””â”€ Listo para detectar 21 landmarks/mano                 â”‚
â”‚                                                              â”‚
â”‚  âœ… Estado: modelReady = true, mediaPipeReady = true       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. INICIO DE DETECCIÃ“N (User clickea "Comenzar")          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  handleStartDetection()                                      â”‚
â”‚   â”œâ”€ Pedir permisos de cÃ¡mara                              â”‚
â”‚   â”œâ”€ setIsDetecting(true)                                  â”‚
â”‚   â”œâ”€ Limpiar frameBuffer y detecciones anteriores          â”‚
â”‚   â””â”€ Iniciar startDetectionLoop()                          â”‚
â”‚                                                              â”‚
â”‚  âœ… Estado: isDetecting = true, frameBuffer = []           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. LOOP DE DETECCIÃ“N (Cada ~33ms @ 30 FPS)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  requestAnimationFrame(startDetectionLoop)                 â”‚
â”‚   â”‚                                                          â”‚
â”‚   â”œâ”€ detectHandsInFrame(videoRef.current)                  â”‚
â”‚   â”‚  â”œâ”€ handDetector.detectForVideo()                      â”‚
â”‚   â”‚  â”œâ”€ Extraer 21 landmarks Ã— 2 manos                     â”‚
â”‚   â”‚  â”œâ”€ Retornar { leftHand, rightHand }                   â”‚
â”‚   â”‚  â””â”€ Tiempo lÃ­mite: 33ms (30 FPS)                       â”‚
â”‚   â”‚                                                          â”‚
â”‚   â”œâ”€ combineHandKeypoints(leftHand, rightHand)            â”‚
â”‚   â”‚  â”œâ”€ Mano izq: Ã­ndices 0-62 (21 Ã— 3)                    â”‚
â”‚   â”‚  â”œâ”€ Mano der: Ã­ndices 63-125 (21 Ã— 3)                  â”‚
â”‚   â”‚  â””â”€ Normalizar a rango [0, 1]                          â”‚
â”‚   â”‚                                                          â”‚
â”‚   â”œâ”€ addFrameToBuffer(keypoints)                           â”‚
â”‚   â”‚  â”œâ”€ Si buffer.length >= 24: remover mÃ¡s viejo          â”‚
â”‚   â”‚  â””â”€ Agregar nuevo frame                                â”‚
â”‚   â”‚                                                          â”‚
â”‚   â”œâ”€ Â¿Buffer.length === 24?                               â”‚
â”‚   â”‚  â”‚                                                      â”‚
â”‚   â”‚  â””â”€ SÃ: predictWithModel(frameBuffer)                  â”‚
â”‚   â”‚     â”œâ”€ Convertir a tensor [1, 24, 126]                â”‚
â”‚   â”‚     â”œâ”€ model.predict(inputTensor)                      â”‚
â”‚   â”‚     â”œâ”€ Encontrar argmax de 67 clases                   â”‚
â”‚   â”‚     â”œâ”€ Mapear Ã­ndice a label                           â”‚
â”‚   â”‚     â”œâ”€ Dispose tensores                                â”‚
â”‚   â”‚     â””â”€ Retornar { word, confidence, ... }              â”‚
â”‚   â”‚                                                          â”‚
â”‚   â””â”€ setDetectedWord() y setConfidence()                   â”‚
â”‚                                                              â”‚
â”‚  âœ… Estado: frameBuffer lleno, detecciÃ³n hecha             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. VISUALIZACIÃ“N (Real-time)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  <DetectionOverlay>                                         â”‚
â”‚   â”œâ”€ Mostrar detectedWord (tamaÃ±o 32px)                    â”‚
â”‚   â”œâ”€ Mostrar confidence (%)                                â”‚
â”‚   â””â”€ Animar si confianza >= 70%                            â”‚
â”‚                                                              â”‚
â”‚  <StatusCard>                                               â”‚
â”‚   â”œâ”€ Barra de progreso del buffer (0-24)                   â”‚
â”‚   â”œâ”€ Estado de TensorFlow.js                               â”‚
â”‚   â”œâ”€ Estado de MediaPipe                                   â”‚
â”‚   â””â”€ Estado de DetecciÃ³n                                   â”‚
â”‚                                                              â”‚
â”‚  âœ… Usuario ve resultado en tiempo real                    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. PARADA DE DETECCIÃ“N (User clickea "Detener")          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  handleStopDetection()                                      â”‚
â”‚   â”œâ”€ setIsDetecting(false)                                 â”‚
â”‚   â”œâ”€ cancelAnimationFrame(animationFrameId)                â”‚
â”‚   â””â”€ Limpiar recursos                                      â”‚
â”‚                                                              â”‚
â”‚  âœ… Estado: isDetecting = false, recursos liberados        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Componentes Clave

### 1ï¸âƒ£ Estado (Lines 77-110)

```javascript
// Modelo y MediaPipe
const [model, setModel] = useState(null);              // TensorFlow model
const [modelReady, setModelReady] = useState(false);   // Â¿EstÃ¡ listo?
const [labels, setLabels] = useState([]);             // 67 etiquetas
const [handDetector, setHandDetector] = useState(null); // MediaPipe detector

// CÃ¡mara
const [cameraPermission, setCameraPermission] = useState(null);
const [isDetecting, setIsDetecting] = useState(false);

// Buffer circular
const [frameBuffer, setFrameBuffer] = useState([]);    // Max 24 frames

// DetecciÃ³n
const [detectedWord, setDetectedWord] = useState(null);
const [confidence, setConfidence] = useState(0);
const [isProcessing, setIsProcessing] = useState(false);

// UI y debugging
const [logs, setLogs] = useState([]);                 // Sistema de logs integrado
```

### 2ï¸âƒ£ NormalizaciÃ³n de Keypoints (Lines 135-145)

```javascript
const normalizeKeypoints = (keypoints) => {
  if (!keypoints) return null;

  // Clamp cada valor al rango [0, 1]
  return keypoints.map((val) =>
    Math.max(0, Math.min(1, val))
  );
};
```

**Por quÃ©:** Los keypoints de MediaPipe vienen en coordenadas normalizadas, pero pueden tener pequeÃ±os errores numÃ©ricos. El clamping asegura que estÃ©n dentro del rango esperado.

### 3ï¸âƒ£ CombinaciÃ³n de Keypoints (Lines 148-178)

```javascript
const combineHandKeypoints = (leftHand, rightHand) => {
  const combined = new Array(126).fill(0);

  // Mano izquierda: Ã­ndices 0-62 (21 landmarks Ã— 3 axes)
  if (leftHand && leftHand.landmarks) {
    let idx = 0;
    leftHand.landmarks.forEach((landmark) => {
      combined[idx++] = landmark.x || 0;
      combined[idx++] = landmark.y || 0;
      combined[idx++] = landmark.z || 0;
    });
  }

  // Mano derecha: Ã­ndices 63-125
  if (rightHand && rightHand.landmarks) {
    let idx = 63;
    rightHand.landmarks.forEach((landmark) => {
      combined[idx++] = landmark.x || 0;
      combined[idx++] = landmark.y || 0;
      combined[idx++] = landmark.z || 0;
    });
  }

  return normalizeKeypoints(combined);
};
```

**Estructura:**
```
Array de 126 elementos:
[0-62]   : Mano izquierda (21 landmarks Ã— 3 axes)
[63-125] : Mano derecha (21 landmarks Ã— 3 axes)
```

### 4ï¸âƒ£ Carga del Modelo (Lines 219-260)

```javascript
const loadTensorFlowModel = async () => {
  // 1. Cargar etiquetas desde JSON
  const loadedLabels = await loadLabelsFromAsset();

  // 2. Cargar modelo LSTM
  const modelAsset = Asset.fromModule(
    require('../assets/model/tfjs_model/model.json')
  );
  const loadedModel = await tf.loadLayersModel(modelAsset.uri);

  // 3. Warmup: predicciÃ³n dummy
  const dummyInput = tf.randomNormal([1, 24, 126]);
  const warmupPred = loadedModel.predict(dummyInput);
  dummyInput.dispose();
  warmupPred.dispose();

  // 4. Marcar como listo
  setModel(loadedModel);
  setModelReady(true);
};
```

**Por quÃ© warmup:**
- Primera predicciÃ³n es lenta (compilaciÃ³n de WebGL)
- Warmup pre-compila para que primera detecciÃ³n sea rÃ¡pida

### 5ï¸âƒ£ DetecciÃ³n de Manos (Lines 288-325)

```javascript
const detectHandsInFrame = async (video) => {
  if (!handDetector || !video) return null;

  try {
    // Control de FPS: mÃ¡ximo 30 FPS (33ms)
    const now = Date.now();
    if (now - lastFrameTime.current < 33) {
      return null;
    }
    lastFrameTime.current = now;

    // Detectar manos
    const detectionResult = await
      handDetector.detectForVideo(video, now);

    // Organizar por izquierda/derecha
    let leftHand = null;
    let rightHand = null;

    detectionResult.handedness.forEach((handedness, idx) => {
      if (handedness[0].categoryName === 'Left') {
        leftHand = { landmarks: detectionResult.landmarks[idx] };
      } else {
        rightHand = { landmarks: detectionResult.landmarks[idx] };
      }
    });

    return { leftHand, rightHand };
  } catch (error) {
    log(`âš ï¸ Error: ${error.message}`);
    return null;
  }
};
```

**FPS Control:** Limita a 30 FPS verificando si han pasado 33ms desde el Ãºltimo frame.

### 6ï¸âƒ£ PredicciÃ³n con el Modelo (Lines 328-380)

```javascript
const predictWithModel = async (buffer) => {
  if (!model || buffer.length < 24) return null;

  try {
    setIsProcessing(true);

    // 1. Convertir buffer a tensor [1, 24, 126]
    const inputTensor = tf.tensor3d([buffer], [1, 24, 126]);

    // 2. PredicciÃ³n
    const outputTensor = model.predict(inputTensor);
    const predictions = await outputTensor.data();
    const predictionsArray = Array.from(predictions);

    // 3. Encontrar mÃ¡xima confianza
    const maxConfidenceIdx = predictionsArray.indexOf(
      Math.max(...predictionsArray)
    );
    const maxConfidence = predictionsArray[maxConfidenceIdx];

    // 4. Mapear a etiqueta
    const predictedLabel = labels[maxConfidenceIdx];

    // 5. Cleanup
    inputTensor.dispose();
    outputTensor.dispose();

    return {
      word: predictedLabel,
      confidence: maxConfidence,
      index: maxConfidenceIdx,
    };
  } catch (error) {
    log(`âŒ Error: ${error.message}`);
    return null;
  }
};
```

---

## ğŸ® CÃ³mo Usar

### Paso 1: Importar en App.js

```javascript
import MonolithicDetectionScreen from './screens/MonolithicDetectionScreen';

// En tu navegaciÃ³n:
<Stack.Screen
  name="MonolithicDetection"
  component={MonolithicDetectionScreen}
/>
```

### Paso 2: Navegar a la pantalla

```javascript
<TouchableOpacity onPress={() => navigation.navigate('MonolithicDetection')}>
  <Text>Abrir MonolÃ­tica</Text>
</TouchableOpacity>
```

### Paso 3: Interactuar

1. **Espera carga:** Modelo + MediaPipe
2. **Click "Comenzar":** Inicia detecciÃ³n
3. **Muestra tu mano:** MediaPipe detecta landmarks
4. **Espera 24 frames:** Buffer se llena
5. **Resultado:** Modelo predice y muestra

---

## ğŸ› Debugging

### Sistema de Logs Integrado

La pantalla tiene un sistema de logging completo SIN dependencias:

```javascript
const log = (message, data = null) => {
  const timestamp = new Date().toLocaleTimeString();
  const logEntry = `[${timestamp}] ${message}`;

  console.log(logEntry);
  setLogs((prevLogs) => [logEntry, ...prevLogs.slice(0, 49)]);
};
```

**CÃ³mo ver logs:**
1. Click en el icono ğŸ› en el header
2. Se abre panel de logs
3. Muestra Ãºltimos 50 logs en tiempo real

### Puntos de Log Clave

```javascript
log('ğŸ“ Cargando etiquetas...');
log(`âœ… ${loadedLabels.length} etiquetas cargadas`);
log('ğŸ“¦ Cargando modelo TensorFlow.js...');
log('âœ… Modelo cargado exitosamente');
log('â–¶ï¸  DetecciÃ³n iniciada');
log(`ğŸ–ï¸ Mano detectada`);
log(`ğŸ¯ DetecciÃ³n: ${predictedLabel} (${confidence}%)`);
log('â¹ï¸  DetecciÃ³n detenida');
```

### Debugging EspecÃ­fico

**Â¿Por quÃ© no detecta manos?**
- Revisa logs: busca "Mano detectada"
- Mejora iluminaciÃ³n
- Coloca mano en centro
- Verifica MediaPipe estÃ¡ listo

**Â¿Por quÃ© predicciÃ³n es lenta?**
- Primera predicciÃ³n es lenta (warmup)
- Siguiente son rÃ¡pidas
- Si sigue lento: reduce resoluciÃ³n video

**Â¿Buffer no se llena?**
- Necesita 24 frames vÃ¡lidos
- A 30 FPS tarda ~800ms
- Revisa logs para ver count

---

## ğŸš€ MigraciÃ³n a ProducciÃ³n

### Paso 1: Extraer Hooks

Crea `hooks/useMonolithicDetection.js`:

```javascript
export const useMonolithicDetection = ({ videoRef, onDetection }) => {
  // Extraer toda la lÃ³gica de inicializaciÃ³n y detecciÃ³n
  // Retornar { startDetection, stopDetection, logs, ... }
};
```

### Paso 2: Crear Servicio

Crea `utils/services/monolithicDetectionService.js`:

```javascript
export class MonolithicDetectionService {
  // Encapsular loadModel(), initMediaPipe(), predictWithModel()
  // Usar listeners en lugar de setState
}
```

### Paso 3: Refactorizar Componente

Simplificar `screens/WordDetectionScreen.mediapipe.js`:

```javascript
const WordDetectionScreen = ({ navigation }) => {
  const { startDetection, stopDetection } = useMonolithicDetection({...});

  return (
    <View>
      <Camera ... />
      {detection && <DetectionOverlay ... />}
      <Button onPress={startDetection} />
    </View>
  );
};
```

### Paso 4: Separar UI y LÃ³gica

Usar patrÃ³n Container/Presentational:

```
MonolithicDetectionContainer.js (lÃ³gica)
  â””â”€ MonolithicDetectionPresenter.js (UI)
```

### Paso 5: Tests

```javascript
test('Model loads correctly', async () => {
  const model = await monolithicDetectionService.loadModel();
  expect(model).toBeDefined();
});

test('Detects 24 frames', async () => {
  const buffer = await monolithicDetectionService.captureFrames(24);
  expect(buffer.length).toBe(24);
});
```

---

## ğŸ“Š ComparaciÃ³n: MonolÃ­tica vs ProducciÃ³n

| Aspecto | MonolÃ­tica | ProducciÃ³n |
|---------|-----------|-----------|
| **Archivo** | 1 | 5+ |
| **Estado** | Todo en componente | Distribuido |
| **Hooks** | Sin hooks abstraÃ­dos | useMediaPipeDetection + useModel |
| **Debugging** | FÃ¡cil (logs integrados) | DifÃ­cil (distribuido) |
| **Testing** | DifÃ­cil (acoplado) | FÃ¡cil (modular) |
| **Rendimiento** | Bueno | Mejor (optimizado) |
| **Mantenibilidad** | â­ | â­â­â­â­â­ |

---

## ğŸ¯ Resumen

`MonolithicDetectionScreen.js` es perfecta para:

âœ… **Entender** cÃ³mo funciona todo el pipeline
âœ… **Debuggear** cada paso de la detecciÃ³n
âœ… **Desarrollar** rÃ¡pidamente nuevas caracterÃ­sticas
âœ… **Demostrar** el flujo completo a otros desarrolladores

âŒ **NO usar en producciÃ³n** - demasiado acoplada
âŒ **NO reutilizar** - cÃ³digo repetido
âŒ **NO escalar** - difÃ­cil de mantener

---

## ğŸ“š Referencias

- [MONOLITHIC_MIGRATION.md](./MONOLITHIC_MIGRATION.md) - GuÃ­a paso a paso
- [MEDIAPIPE_INTEGRATION.md](./MEDIAPIPE_INTEGRATION.md) - MediaPipe especÃ­fico
- [PIPELINE_VALIDATION.js](./PIPELINE_VALIDATION.js) - ValidaciÃ³n del pipeline

---

**Status:** âœ… Listo para usar
**Ãšltima actualizaciÃ³n:** 2025-11-13
**Autor:** SignBridge Dev Team
